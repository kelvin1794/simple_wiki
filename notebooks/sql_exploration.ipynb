{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Exploration of SQL files\n",
    "\n",
    "### Programmatically download the SQL dumps\n",
    "\n",
    "Let's first look at the available versions of the database"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['../',\n",
       " '20200520/',\n",
       " '20200601/',\n",
       " '20200620/',\n",
       " '20200701/',\n",
       " '20200720/',\n",
       " '20200801/',\n",
       " '20200820/',\n",
       " '20200901/',\n",
       " '20200920/',\n",
       " '20201001/',\n",
       " '20201020/',\n",
       " 'latest/']"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://dumps.wikimedia.org/simplewiki/\"\n",
    "# Get the text text response of the base page\n",
    "index = requests.get(base_url).text\n",
    "soup_index = BeautifulSoup(index, \"html.parser\")\n",
    "\n",
    "# In the base index, there should be multiple <a> tag leading\n",
    "# to different version of the database\n",
    "dumps = [a[\"href\"] for a in soup_index.find_all(\"a\") if a.has_attr(\"href\")]\n",
    "\n",
    "dumps"
   ]
  },
  {
   "source": [
    "As we can see, the last page is `lastest/` and this link might contains files with different timestamp because it's updated incrementally when a dump is being processed.\n",
    "\n",
    "As of the time this notebook is being written, `20201020/` is being generated so if we look into `latest/`, we would see some files were updated on `20201001/` and some other were updated on `20201020/`.\n",
    "\n",
    "For simplicity, while the newest dump is being generated, let's hardcode the timestamp and we will change it to a dynamic variable later on production.\n",
    "\n",
    "For a more complex hanlder, we can always look for `<li>` tag with the word \"waiting\", meaning the version is being processed and we can schedule the task some time later.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"latest\" not in dumps[-1]:\n",
    "    print(\"Couldn't find the latest dump\")\n",
    "    exit\n",
    "\n",
    "# Later on production, we will use this.\n",
    "dump = dumps[-2]\n",
    "\n",
    "# For now, let's use this.\n",
    "dump = \"20201001/\""
   ]
  },
  {
   "source": [
    "Once we have that, we can now look into a specific version, exact all `<li>` tag with `class=\"file\"` to get all the file links. We can also filter everything else out, except `SQL` file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dump url with the base and the latest timestamp\n",
    "dump_url = base_url + dump\n",
    "\n",
    "# Retrieve the dump page\n",
    "dump_html = requests.get(dump_url).text\n",
    "soup_dump = BeautifulSoup(dump_html, \"html.parser\")\n",
    "\n",
    "# Search for SQL files\n",
    "files = []\n",
    "for file in soup_dump.find_all(\"li\", {\"class\": \"file\"}):\n",
    "    text = file.text\n",
    "    if \"sql\" in text:\n",
    "        files.append((text.split()[0], text.split()[1:]))\n",
    "\n",
    "files_to_download = [file[0] for file in files]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('simplewiki-20201001-image.sql.gz', ['5', 'KB']),\n",
       " ('simplewiki-20201001-category.sql.gz', ['539', 'KB']),\n",
       " ('simplewiki-20201001-user_groups.sql.gz', ['3', 'KB']),\n",
       " ('simplewiki-20201001-page_props.sql.gz', ['5.7', 'MB']),\n",
       " ('simplewiki-20201001-iwlinks.sql.gz', ['1.8', 'MB']),\n",
       " ('simplewiki-20201001-site_stats.sql.gz', ['796', 'bytes']),\n",
       " ('simplewiki-20201001-wbc_entity_usage.sql.gz', ['5.1', 'MB']),\n",
       " ('simplewiki-20201001-externallinks.sql.gz', ['27.2', 'MB']),\n",
       " ('simplewiki-20201001-change_tag_def.sql.gz', ['1', 'KB']),\n",
       " ('simplewiki-20201001-page_restrictions.sql.gz', ['21', 'KB']),\n",
       " ('simplewiki-20201001-pagelinks.sql.gz', ['58.2', 'MB']),\n",
       " ('simplewiki-20201001-babel.sql.gz', ['3', 'KB']),\n",
       " ('simplewiki-20201001-sites.sql.gz', ['20', 'KB']),\n",
       " ('simplewiki-20201001-page.sql.gz', ['18.8', 'MB']),\n",
       " ('simplewiki-20201001-user_former_groups.sql.gz', ['2', 'KB']),\n",
       " ('simplewiki-20201001-protected_titles.sql.gz', ['15', 'KB']),\n",
       " ('simplewiki-20201001-redirect.sql.gz', ['811', 'KB']),\n",
       " ('simplewiki-20201001-geo_tags.sql.gz', ['435', 'KB']),\n",
       " ('simplewiki-20201001-change_tag.sql.gz', ['6.9', 'MB']),\n",
       " ('simplewiki-20201001-imagelinks.sql.gz', ['5.7', 'MB']),\n",
       " ('simplewiki-20201001-templatelinks.sql.gz', ['22.4', 'MB']),\n",
       " ('simplewiki-20201001-categorylinks.sql.gz', ['19.4', 'MB']),\n",
       " ('simplewiki-20201001-langlinks.sql.gz', ['69.9', 'MB'])]"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "source": [
    "Once we have determined the files needed to be downloaded, we can use `Keras`'s `get_file` utility which is extremely handy to download a file and save to disk."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading simplewiki-20201001-image.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-image.sql.gz\n",
      "8192/5596 [===========================================] - 0s 0us/step\n",
      "0.005596\n",
      "Downloading simplewiki-20201001-category.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-category.sql.gz\n",
      "557056/552897 [==============================] - 2s 3us/step\n",
      "0.552897\n",
      "Downloading simplewiki-20201001-user_groups.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-user_groups.sql.gz\n",
      "8192/3175 [=============================================================================] - 0s 0us/step\n",
      "0.003175\n",
      "Downloading simplewiki-20201001-page_props.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-page_props.sql.gz\n",
      "5955584/5954045 [==============================] - 11s 2us/step\n",
      "5.954045\n",
      "Downloading simplewiki-20201001-iwlinks.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-iwlinks.sql.gz\n",
      "1843200/1837306 [==============================] - 6s 4us/step\n",
      "1.837306\n",
      "Downloading simplewiki-20201001-site_stats.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-site_stats.sql.gz\n",
      "8192/796 [====================================================================================================================================================================================================================================================================================================================] - 0s 0us/step\n",
      "0.000796\n",
      "Downloading simplewiki-20201001-wbc_entity_usage.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-wbc_entity_usage.sql.gz\n",
      "5349376/5342004 [==============================] - 10s 2us/step\n",
      "5.342004\n",
      "Downloading simplewiki-20201001-externallinks.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-externallinks.sql.gz\n",
      "28483584/28475883 [==============================] - 15s 1us/step\n",
      "28.475883\n",
      "Downloading simplewiki-20201001-change_tag_def.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-change_tag_def.sql.gz\n",
      "8192/2044 [========================================================================================================================] - 0s 0us/step\n",
      "0.002044\n",
      "Downloading simplewiki-20201001-page_restrictions.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-page_restrictions.sql.gz\n",
      "24576/22218 [=================================] - 0s 0us/step\n",
      "0.022218\n",
      "Downloading simplewiki-20201001-pagelinks.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-pagelinks.sql.gz\n",
      "60981248/60978453 [==============================] - 20s 0us/step\n",
      "60.978453\n",
      "Downloading simplewiki-20201001-babel.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-babel.sql.gz\n",
      "8192/3298 [==========================================================================] - 0s 0us/step\n",
      "0.003298\n",
      "Downloading simplewiki-20201001-sites.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-sites.sql.gz\n",
      "24576/21212 [==================================] - 0s 0us/step\n",
      "0.021212\n",
      "Downloading simplewiki-20201001-page.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-page.sql.gz\n",
      "19701760/19695105 [==============================] - 15s 1us/step\n",
      "19.695105\n",
      "Downloading simplewiki-20201001-user_former_groups.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-user_former_groups.sql.gz\n",
      "8192/2321 [=========================================================================================================] - 0s 0us/step\n",
      "0.002321\n",
      "Downloading simplewiki-20201001-protected_titles.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-protected_titles.sql.gz\n",
      "16384/15588 [===============================] - 0s 0us/step\n",
      "0.015588\n",
      "Downloading simplewiki-20201001-redirect.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-redirect.sql.gz\n",
      "835584/830593 [==============================] - 5s 5us/step\n",
      "0.830593\n",
      "Downloading simplewiki-20201001-geo_tags.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-geo_tags.sql.gz\n",
      "450560/445834 [==============================] - 2s 5us/step\n",
      "0.445834\n",
      "Downloading simplewiki-20201001-change_tag.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-change_tag.sql.gz\n",
      "7233536/7230065 [==============================] - 11s 1us/step\n",
      "7.230065\n",
      "Downloading simplewiki-20201001-imagelinks.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-imagelinks.sql.gz\n",
      "6004736/6002131 [==============================] - 11s 2us/step\n",
      "6.002131\n",
      "Downloading simplewiki-20201001-templatelinks.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-templatelinks.sql.gz\n",
      "23445504/23443406 [==============================] - 16s 1us/step\n",
      "23.443406\n",
      "Downloading simplewiki-20201001-categorylinks.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-categorylinks.sql.gz\n",
      "20373504/20367279 [==============================] - 14s 1us/step\n",
      "20.367279\n",
      "Downloading simplewiki-20201001-langlinks.sql.gz ...\n",
      "Downloading data from https://dumps.wikimedia.org/simplewiki/20201001/simplewiki-20201001-langlinks.sql.gz\n",
      "73261056/73254345 [==============================] - 33s 0us/step\n",
      "73.254345\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import get_file\n",
    "from os import path\n",
    "import subprocess\n",
    "\n",
    "# Directory where keras download the files\n",
    "dataset_dir = os.path.join(os.getcwd(), \"datasets\")\n",
    "\n",
    "data_paths = []\n",
    "file_info = []\n",
    "\n",
    "for file in files_to_download:\n",
    "    path = os.path.join(dataset_dir, file)\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Downloading {file} ...\")\n",
    "        data_paths.append(\n",
    "            get_file(fname=file, origin=dump_url + file, cache_subdir=dataset_dir)\n",
    "        )\n",
    "        # Find the file size in MB\n",
    "        file_size = os.stat(path).st_size / 1e6\n",
    "        print(file_size)\n",
    "\n",
    "        file_info.append((file, file_size))\n",
    "\n",
    "    else:\n",
    "        # If file exists, put in the list still, for later processing\n",
    "        data_paths.append(path)\n",
    "        file_size = os.stat(path).st_size / 1e6\n",
    "        file_info.append((file.split(\"-\")[-1], file_size))\n",
    "\n",
    "for data_path in data_paths:\n",
    "    subprocess.call([\"gunzip\", f\"{data_path}\"])"
   ]
  },
  {
   "source": [
    "After the step above, we should now have all the `.sql` files under the folder `datasets`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the paths of all sql files\n",
    "sql_data_paths = [item[:-3] for item in data_paths]\n",
    "\n",
    "# Run mysql command import the files\n",
    "# It's okay to expose the password since it's a Docker\n",
    "# container running on our local machine\n",
    "for sql_data_path in sql_data_paths:\n",
    "    with open(sql_data_path) as f:\n",
    "        command = [\n",
    "            \"mysql\",\n",
    "            f\"--host=b\",\n",
    "            f\"--user=root\",\n",
    "            f\"--password=LocalPassword\",\n",
    "            f\"--database=simplewiki\",\n",
    "        ]\n",
    "        p = subprocess.Popen(\n",
    "            command,\n",
    "            stdin=f,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        output, error = p.communicate()\n",
    "        if p.returncode != 0:\n",
    "            # We can handle this better by sending an email or similar\n",
    "            print(error)\n",
    "            "
   ]
  },
  {
   "source": [
    "Lastly, clean up the files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# Once done with the data import, clean things up.\n",
    "os.system(\"rm datasets/*\")\n",
    "os.system(\"rm -rf datasets\")"
   ]
  },
  {
   "source": [
    "With that, we should now be able to refactor the code to run as a function of our application."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}